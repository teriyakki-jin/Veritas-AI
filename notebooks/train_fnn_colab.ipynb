{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# FakeNewsNet / WELFake Training v2 (2-class DistilBERT)\n**Dataset:** WELFake (57,707 train / 14,427 test)\n**Task:** 2-class fake news detection\n**Labels:** fake (0), real (1)\n\n### v2 개선사항\n- cosine LR scheduler\n- early stopping (patience=2)\n- confusion matrix 시각화\n\n> 현재 모델 성능: Acc 99.2% / F1 99.2% (이미 우수)\n> Runtime → GPU (T4) 설정 필수"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q transformers datasets accelerate scikit-learn safetensors"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "PROJECT_DIR = '/content/drive/MyDrive/Fakenews-detect'\n",
    "os.makedirs(f'{PROJECT_DIR}/models/fakenewsnet_baseline', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/data/welfake', exist_ok=True)\n",
    "print('Project dir:', PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Data\n",
    "`data/welfake/train.jsonl`, `test.jsonl`을 Google Drive의 `Fakenews-detect/data/welfake/`에 업로드하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "data_dir = f'{PROJECT_DIR}/data/welfake'\n",
    "needed = ['train.jsonl', 'test.jsonl']\n",
    "missing = [f for f in needed if not os.path.exists(os.path.join(data_dir, f))]\n",
    "\n",
    "if missing:\n",
    "    print(f'Missing: {missing}. Upload now:')\n",
    "    uploaded = files.upload()\n",
    "    for fname, content in uploaded.items():\n",
    "        dest = os.path.join(data_dir, fname)\n",
    "        with open(dest, 'wb') as f:\n",
    "            f.write(content)\n",
    "        print(f'  Saved: {dest}')\n",
    "else:\n",
    "    print('All data files present!')\n",
    "\n",
    "for f in needed:\n",
    "    p = os.path.join(data_dir, f)\n",
    "    if os.path.exists(p):\n",
    "        with open(p, 'r') as fh:\n",
    "            n = sum(1 for _ in fh)\n",
    "        print(f'  {f}: {n:,} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport json\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n\nLABEL_NAMES = ['fake', 'real']\nLABEL_MAP = {'fake': 0, 'real': 1}\n\nclass FNNDataset(Dataset):\n    def __init__(self, data_path, tokenizer, max_len=256):\n        self.samples = []\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        with open(data_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                self.samples.append(json.loads(line))\n        print(f'Loaded {len(self.samples)} samples from {data_path}')\n\n        from collections import Counter\n        dist = Counter(s.get('label_class', '?') for s in self.samples)\n        print(f'Label distribution: {dict(dist)}')\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        item = self.samples[idx]\n        text = str(item.get('text', '') or item.get('title', ''))\n        label_str = item.get('label_class', 'fake')\n        label_id = LABEL_MAP.get(label_str, 0)\n\n        encoding = self.tokenizer(\n            text, max_length=self.max_len, padding='max_length',\n            truncation=True, return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label_id, dtype=torch.long)\n        }\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return {\n        'accuracy': accuracy_score(labels, preds),\n        'f1_macro': f1_score(labels, preds, average='macro')\n    }\n\nprint('Device:', 'cuda' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.is_available():\n    print('GPU:', torch.cuda.get_device_name(0))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MODEL_NAME = 'distilbert-base-uncased'\nDATA_DIR = f'{PROJECT_DIR}/data/welfake'\nOUTPUT_DIR = f'{PROJECT_DIR}/models/fakenewsnet_baseline'\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n\ntrain_dataset = FNNDataset(f'{DATA_DIR}/train.jsonl', tokenizer, max_len=256)\ntest_dataset = FNNDataset(f'{DATA_DIR}/test.jsonl', tokenizer, max_len=256)\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    warmup_ratio=0.06,\n    weight_decay=0.01,\n    learning_rate=2e-5,\n    lr_scheduler_type='cosine',\n    logging_steps=100,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='f1_macro',\n    greater_is_better=True,\n    fp16=torch.cuda.is_available(),\n    dataloader_num_workers=2,\n    report_to='none',\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n)\n\nprint(f'Training: {len(train_dataset):,} samples, 3 epochs (with early stopping)')\ntrainer.train()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results = trainer.evaluate(test_dataset)\nprint('Test Results:', results)\n\npreds_output = trainer.predict(test_dataset)\npreds = np.argmax(preds_output.predictions, axis=-1)\nlabels = preds_output.label_ids\n\nprint('\\n=== Classification Report ===')\nprint(classification_report(labels, preds, target_names=LABEL_NAMES))\n\ncm = confusion_matrix(labels, preds)\nprint('=== Confusion Matrix ===')\nprint(cm)\n\n# Visualize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfig, ax = plt.subplots(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=ax)\nax.set_xlabel('Predicted')\nax.set_ylabel('True')\nax.set_title(f'FNN Test - Acc: {results[\"eval_accuracy\"]:.3f} / F1: {results[\"eval_f1_macro\"]:.3f}')\nplt.tight_layout()\nplt.savefig(f'{OUTPUT_DIR}/confusion_matrix.png', dpi=150)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/label_map.json', 'w') as f:\n",
    "    json.dump(LABEL_MAP, f)\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/test_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f'Model saved to {OUTPUT_DIR}')\n",
    "print('Files:', os.listdir(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd \"{OUTPUT_DIR}\" && zip -r /content/fakenewsnet_baseline.zip config.json model.safetensors tokenizer.json tokenizer_config.json special_tokens_map.json vocab.txt label_map.json test_results.json 2>/dev/null || echo 'Some files may not exist yet'\n",
    "from google.colab import files\n",
    "files.download('/content/fakenewsnet_baseline.zip')"
   ]
  }
 ]
}
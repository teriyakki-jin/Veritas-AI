{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# FEVER Baseline Training v2 (3-class DistilBERT)\n**Dataset:** FEVER (145,449 train, augmented with evidence)\n**Task:** 3-class claim verification\n**Labels:** SUPPORTS, REFUTES, NOT ENOUGH INFO\n\n### v2 개선사항\n- cosine LR scheduler + warmup ratio\n- epoch 4 + early stopping (patience=2)\n- label smoothing 0.05\n- confusion matrix 시각화\n\n> Runtime → GPU (T4) 설정 필수"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q transformers datasets accelerate scikit-learn safetensors"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "PROJECT_DIR = '/content/drive/MyDrive/Fakenews-detect'\n",
    "os.makedirs(f'{PROJECT_DIR}/models/fever_baseline', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/data/fever', exist_ok=True)\n",
    "print('Project dir:', PROJECT_DIR)\n",
    "print('Files in data/fever:', [f for f in os.listdir(f'{PROJECT_DIR}/data/fever') if f.endswith('.jsonl')] if os.path.exists(f'{PROJECT_DIR}/data/fever') else 'NOT FOUND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Data\n",
    "`data/fever/train_normalized.jsonl`을 Google Drive의 `Fakenews-detect/data/fever/`에 업로드하세요.\n",
    "\n",
    "(augmented 버전이 있으면 `train_augmented.jsonl`을 우선 사용합니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "data_dir = f'{PROJECT_DIR}/data/fever'\n",
    "aug_path = os.path.join(data_dir, 'train_augmented.jsonl')\n",
    "norm_path = os.path.join(data_dir, 'train_normalized.jsonl')\n",
    "\n",
    "if os.path.exists(aug_path):\n",
    "    TRAIN_PATH = aug_path\n",
    "    print(f'Using augmented data: {aug_path}')\n",
    "elif os.path.exists(norm_path):\n",
    "    TRAIN_PATH = norm_path\n",
    "    print(f'Using normalized data: {norm_path}')\n",
    "else:\n",
    "    print('No FEVER data found. Upload train_normalized.jsonl:')\n",
    "    uploaded = files.upload()\n",
    "    for fname, content in uploaded.items():\n",
    "        dest = os.path.join(data_dir, fname)\n",
    "        with open(dest, 'wb') as f:\n",
    "            f.write(content)\n",
    "        print(f'  Saved: {dest}')\n",
    "    TRAIN_PATH = os.path.join(data_dir, list(uploaded.keys())[0])\n",
    "\n",
    "with open(TRAIN_PATH, 'r') as f:\n",
    "    n = sum(1 for _ in f)\n",
    "print(f'Train samples: {n:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport json\nimport numpy as np\nfrom torch.utils.data import Dataset, random_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n\nLABEL_NAMES = ['SUPPORTS', 'REFUTES', 'NOT ENOUGH INFO']\nLABEL_MAP = {name: i for i, name in enumerate(LABEL_NAMES)}\n\nclass FEVERDataset(Dataset):\n    def __init__(self, data_path, tokenizer, max_len=256):\n        self.samples = []\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        with open(data_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                self.samples.append(json.loads(line))\n        print(f'Loaded {len(self.samples)} samples from {data_path}')\n\n        from collections import Counter\n        dist = Counter(s.get('label_class', '?') for s in self.samples)\n        print(f'Label distribution: {dict(dist)}')\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        item = self.samples[idx]\n        claim = str(item['text'])\n\n        evidence_texts = item.get('evidence_texts', [])\n        if evidence_texts:\n            input_text = claim + ' [SEP] ' + ' [SEP] '.join(evidence_texts[:3])\n        else:\n            input_text = claim\n\n        label_str = item.get('label_class', 'NOT ENOUGH INFO')\n        label_id = LABEL_MAP.get(label_str, 2)\n\n        encoding = self.tokenizer(\n            input_text, max_length=self.max_len, padding='max_length',\n            truncation=True, return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label_id, dtype=torch.long)\n        }\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return {\n        'accuracy': accuracy_score(labels, preds),\n        'f1_macro': f1_score(labels, preds, average='macro')\n    }\n\nprint('Device:', 'cuda' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.is_available():\n    print('GPU:', torch.cuda.get_device_name(0))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MODEL_NAME = 'distilbert-base-uncased'\nOUTPUT_DIR = f'{PROJECT_DIR}/models/fever_baseline'\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n\n# Load and split\nfull_dataset = FEVERDataset(TRAIN_PATH, tokenizer, max_len=256)\n\nval_path = os.path.join(data_dir, 'validation_normalized.jsonl')\nif os.path.exists(val_path):\n    train_dataset = full_dataset\n    valid_dataset = FEVERDataset(val_path, tokenizer, max_len=256)\n    print(f'Using separate validation file: {len(valid_dataset)} samples')\nelse:\n    total = len(full_dataset)\n    train_size = int(0.9 * total)\n    val_size = total - train_size\n    train_dataset, valid_dataset = random_split(full_dataset, [train_size, val_size],\n                                                 generator=torch.Generator().manual_seed(42))\n    print(f'Split: train={train_size:,}, valid={val_size:,}')\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=4,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    warmup_ratio=0.06,\n    weight_decay=0.01,\n    learning_rate=3e-5,\n    lr_scheduler_type='cosine',\n    label_smoothing_factor=0.05,\n    logging_steps=200,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='f1_macro',\n    greater_is_better=True,\n    fp16=torch.cuda.is_available(),\n    dataloader_num_workers=2,\n    report_to='none',\n    save_total_limit=2,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n)\n\nprint(f'Training: {len(train_dataset):,} samples, 4 epochs (with early stopping)')\nprint(f'v2 improvements: cosine LR, label smoothing 0.05')\ntrainer.train()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results = trainer.evaluate(valid_dataset)\nprint('Validation Results:', results)\n\n# Detailed report\npreds_output = trainer.predict(valid_dataset)\npreds = np.argmax(preds_output.predictions, axis=-1)\nlabels = preds_output.label_ids\n\nprint('\\n=== Classification Report ===')\nprint(classification_report(labels, preds, target_names=LABEL_NAMES))\n\ncm = confusion_matrix(labels, preds)\nprint('=== Confusion Matrix ===')\nprint(cm)\n\n# Visualize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfig, ax = plt.subplots(figsize=(7, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=ax)\nax.set_xlabel('Predicted')\nax.set_ylabel('True')\nax.set_title(f'FEVER Eval - Acc: {results[\"eval_accuracy\"]:.3f} / F1: {results[\"eval_f1_macro\"]:.3f}')\nplt.tight_layout()\nplt.savefig(f'{OUTPUT_DIR}/confusion_matrix.png', dpi=150)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/label_map.json', 'w') as f:\n",
    "    json.dump(LABEL_MAP, f)\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/eval_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f'Model saved to {OUTPUT_DIR}')\n",
    "print('Files:', os.listdir(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd \"{OUTPUT_DIR}\" && zip -r /content/fever_baseline.zip config.json model.safetensors tokenizer.json tokenizer_config.json special_tokens_map.json vocab.txt label_map.json eval_results.json 2>/dev/null || echo 'Some files may not exist yet'\n",
    "from google.colab import files\n",
    "files.download('/content/fever_baseline.zip')"
   ]
  }
 ]
}